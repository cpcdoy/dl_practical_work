{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probabilistic Models (DDPM) Practical Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This forces Jupyter to reload all `.py` files that you are using on the side. Otherrwise it'll load them once and if you modify the code in the your `.py` files you'll have to reload your kernel for the changes to be reloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dagster 1.5.3 requires sqlalchemy>=1.0, but you have sqlalchemy 0.7.10 which is incompatible.\n",
      "tokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.23.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U einops datasets matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cpcdoy/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load dataset from the hub\n",
    "dataset = load_dataset(\"fashion_mnist\")\n",
    "image_size = 28\n",
    "channels = 1\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "# define image transformations (e.g. using torchvision)\n",
    "transform = Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda t: (t * 2) - 1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# define function\n",
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [transform(image) for image in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "transformed_dataset = dataset.with_transform(transforms).remove_columns(\"label\")\n",
    "\n",
    "# create dataloader\n",
    "dataloader = DataLoader(\n",
    "    transformed_dataset[\"train\"], batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Denoising Diffusion Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Beta Schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Beta Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Beta Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
    "    \"\"\"\n",
    "\n",
    "    # COMPLETE THIS\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 600\n",
    "\n",
    "# define beta schedule\n",
    "betas = linear_beta_schedule(timesteps=timesteps) # or `cosine_beta_schedule(timesteps=timesteps)`\n",
    "\n",
    "# define alphas\n",
    "alphas = ...\n",
    "alphas_cumprod = ...\n",
    "\n",
    "# This is just the previous step of the cumulative product above\n",
    "# It's just alphas_cumprod without the last value and with a 1.0 padding at the beginning\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "\n",
    "sqrt_recip_alphas = ...\n",
    "\n",
    "# calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "sqrt_alphas_cumprod = ...\n",
    "sqrt_one_minus_alphas_cumprod = ...\n",
    "\n",
    "# calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "posterior_variance = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def p_sample(model, x_t, ts, current_t):\n",
    "    \"\"\"\n",
    "    model: Our model we'll create later\n",
    "    x_t: The noisy image of current time_step `t`\n",
    "    ts: All the $t$ for the current time step, basically an array with only `t` times the batch size. Remember that we are always computing our formulas for multiple images at the same time (aka all imaages in the batch).\n",
    "    current_t: The $t$ integer value from the `ts` array. It's more convenient to have by itself if we want to do the if condition we saw. You could also take the first (or any other) value from the `ts` array, but less convenient.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the current time step constants `*_t` here\n",
    "\n",
    "    # COMPLETE THIS\n",
    "    sqrt_recip_alphas_t = ...\n",
    "    betas_t = ...\n",
    "    sqrt_one_minus_alphas_cumprod_t = ...\n",
    "\n",
    "    mean_t = ...\n",
    "\n",
    "    # The condition line 3 in the algorithm\n",
    "    if current_t == 0:\n",
    "        # `if t = 0: z = 0` so we can just return the `mean_t`\n",
    "        return mean_t\n",
    "    else:\n",
    "        # COMPLETE THIS\n",
    "        posterior_variance_t = ...\n",
    "        z = ...\n",
    "\n",
    "        return mean_t + ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def p_sample_loop(model, shape):\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    batch_size = shape[0]\n",
    "    # start from pure noise (for each example in the batch)\n",
    "    img = torch.randn(shape, device=device)\n",
    "    imgs = []\n",
    "\n",
    "    for t in tqdm(\n",
    "        reversed(range(0, timesteps)), desc=\"sampling loop time step\", total=timesteps\n",
    "    ):\n",
    "        # torch.full: Creates a tensor of size size filled with value i\n",
    "        img = p_sample(\n",
    "            model, img, torch.full((batch_size,), t, device=device, dtype=torch.long), t\n",
    "        )\n",
    "        imgs.append(img.cpu().numpy())\n",
    "    return imgs\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, image_size, batch_size=16, channels=3):\n",
    "    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_transform_tensor_to_pil_image\n",
    "\n",
    "\n",
    "# forward diffusion\n",
    "def q_sample(x_0, ts, noise=None):\n",
    "    \"\"\"\n",
    "    x_0: The original image that we want to add noise to given the specific beta schedule we precomputed above\n",
    "    ts: All the $t$ for the current time step, basically an array with only `t` times the batch size. Remember that we are always computing our formulas for multiple images at the same time (aka all imaages in the batch).\n",
    "    \"\"\"\n",
    "\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_0)\n",
    "\n",
    "    # COMPLETE THIS\n",
    "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, ts, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, ts, x_0.shape\n",
    "    )\n",
    "\n",
    "    # The red rectangle part in our formula\n",
    "    model_input = ...\n",
    "\n",
    "    return model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is already made for you, it computes the full loss from the training loop above using your implementation of `q_sample` (the red rectangle part)\n",
    "# You can choose between 3 loss types, \"l1\", \"l2\" (or Mean Squared Error (MSE), like in the paper) or \"huber\" (or smooth l1) loss.\n",
    "def p_losses(denoise_model, x_0, t, noise=None, loss_type=\"l1\"):\n",
    "    # The noise `epsilon` in our equation to which we compare our model noise prediction\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_0)\n",
    "\n",
    "    # This is where `q_sample` is being used\n",
    "    # `x_noisy` is basically our model input\n",
    "    x_noisy = q_sample(x_0=x_0, t=t, noise=noise)\n",
    "\n",
    "    # epsilon_theta from our formula in the green rectangle\n",
    "    predicted_noise = denoise_model(x_noisy, t)\n",
    "\n",
    "    # The `|| epsilon - epsilon_theta ||^2` part of the equation\n",
    "    # The derivative part is only computed later in the training loop by PyTorch as we've been doing for all our models up until now\n",
    "    # You can choose between 3 losses, L2/MSE loss is the one from the paper\n",
    "    if loss_type == \"l1\":\n",
    "        # Same as L1 without the power of 2\n",
    "        loss = F.l1_loss(noise, predicted_noise)\n",
    "    elif loss_type == \"l2\":\n",
    "        # The loss in the paper\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "    elif loss_type == \"huber\":\n",
    "        # The Huber loss might be slightly better in this case\n",
    "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
    "    else:\n",
    "        # If we input any another loss\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # Return the final loss value\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "from model import Unet\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = Unet(\n",
    "    dim=image_size,\n",
    "    channels=channels,\n",
    "    dim_mults=(1, 2, 4,),\n",
    "    use_convnext=False, # You can experiment with the other architecture that uses ConvNext\n",
    "    resnet_block_groups=1, # Set this to 1 for ResNet and 8 for ConvNext\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_transform_tensor_to_pil_image\n",
    "\n",
    "reverse_transform = generate_transform_tensor_to_pil_image()\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_size = batch[\"pixel_values\"].shape[0]\n",
    "        batch = batch[\"pixel_values\"].to(device)\n",
    "\n",
    "        # Generate time steps t uniformally (from 0 to timesteps=600 we defined above) for every image in the batch\n",
    "        t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
    "\n",
    "        loss = p_losses(model, batch, t, loss_type=\"huber\")\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(\"Loss:\", loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "       \n",
    "    # sample 4 images\n",
    "    samples = sample(\n",
    "        model, image_size=image_size, batch_size=4, channels=channels\n",
    "    )\n",
    "\n",
    "    # show random ones during train\n",
    "    plt.title(f\"Epoch {epoch}, step {step}, loss {loss.item()}\")\n",
    "    for i in range(4):\n",
    "        plt.imshow(\n",
    "            reverse_transform(torch.from_numpy(samples[-1][i])), cmap=\"gray\"\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample images\n",
    "bs = 32\n",
    "\n",
    "samples = sample(model, image_size=image_size, batch_size=bs, channels=channels, cmap='gray')\n",
    "\n",
    "# show random ones\n",
    "for i in range(bs):\n",
    "    plt.imshow(reverse_transform(torch.from_numpy(samples[-1][i])))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
